{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Preamble\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as IO\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from scipy.signal import convolve\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"deeptrack/\")\n",
    "import deeptrack as dt\n",
    "import deeptrack.models\n",
    "from deeptrack.features import Feature\n",
    "import warnings\n",
    "from deeptrack.losses import weighted_crossentropy\n",
    "import keras\n",
    "K = keras.backend\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Define cGAN model\n",
    "baseSize = 8 #default 8\n",
    "DSFac = 5 #default 5\n",
    "\n",
    "conv_layers_dimensions = tuple([baseSize*2**i for i in range(0, DSFac)])\n",
    "\n",
    "def init_model():\n",
    "    weight_init = RandomNormal(mean = 0.0, stddev = 0.02)\n",
    "    activation = lambda x: layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    convolution_block = dt.layers.ConvolutionalBlock(\n",
    "        activation=activation, \n",
    "        instance_norm=True,\n",
    "        kernel_initializer=weight_init\n",
    "    )\n",
    "    base_block = dt.layers.ResidualBlock(\n",
    "        activation=activation,\n",
    "        instance_norm=True,\n",
    "        kernel_initializer=weight_init\n",
    "    )\n",
    "    pooling_block = dt.layers.ConvolutionalBlock(\n",
    "        strides=2, \n",
    "        activation=activation, \n",
    "        instance_norm=True,\n",
    "        kernel_initializer=weight_init\n",
    "    )\n",
    "    deconvolution_block = dt.layers.StaticUpsampleBlock(\n",
    "        kernel_size=3, \n",
    "        instance_norm=True,\n",
    "        activation=activation)\n",
    "    \n",
    "    # unet generator\n",
    "    generator = dt.models.unet((None,None,1),conv_layers_dimensions=conv_layers_dimensions,steps_per_pooling=2,output_activation=\"sigmoid\")\n",
    "\n",
    "    discriminator_convolution_block = dt.layers.ConvolutionalBlock(\n",
    "        kernel_size=(4, 4), \n",
    "        strides=1, \n",
    "        activation=activation,\n",
    "        instance_norm=lambda x: (print(x), False if x==16 else {\"axis\":-1, \"center\": False, \"scale\":False})[1]\n",
    "    )\n",
    "\n",
    "    discriminator_pooling_block = dt.layers.ConvolutionalBlock(\n",
    "        kernel_size=(4, 4), \n",
    "        strides=2, \n",
    "        activation=activation, \n",
    "        instance_norm={\"axis\":-1, \"center\": False, \"scale\":False},    \n",
    "    )\n",
    "\n",
    "    discriminator = dt.models.convolutional(\n",
    "        input_shape = [(reduced_times, reduced_length, 1),]*2,  \n",
    "        dropout = [0,0], #default 0.2\n",
    "        conv_layers_dimensions = (16, 32, 64, 128, 256),   # number of features in each convolutional layer\n",
    "        dense_layers_dimensions = (32,32),                 # number of neurons in each dense layer\n",
    "        number_of_outputs = 1,                             # number of neurons in the final dense step (numebr of output values)\n",
    "        compile = False,\n",
    "        output_kernel_size = 4,\n",
    "        convolution_block=discriminator_convolution_block,\n",
    "        pooling_block=discriminator_pooling_block   \n",
    "    )\n",
    "    return discriminator, generator\n",
    "\n",
    "def _compile(\n",
    "    model: models.Model, *, loss=\"mae\", optimizer=\"adam\", metrics=[], **kwargs\n",
    "):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Model(Feature):\n",
    "    def __init__(self, model, **kwargs):\n",
    "        self.model = model\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        # Allows access to the model methods and properties\n",
    "        try:\n",
    "            return getattr(super(), key)\n",
    "        except AttributeError:\n",
    "            return getattr(self.model, key)\n",
    "\n",
    "\n",
    "class cgan(Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        generator=None,\n",
    "        discriminator=None,\n",
    "        discriminator_loss=None,\n",
    "        discriminator_optimizer=None,\n",
    "        discriminator_metrics=None,\n",
    "        assemble_loss=None,\n",
    "        assemble_optimizer=None,\n",
    "        assemble_loss_weights=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator.compile(\n",
    "            loss=discriminator_loss,\n",
    "            optimizer=discriminator_optimizer,\n",
    "            metrics=discriminator_metrics,\n",
    "        )\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = generator\n",
    "\n",
    "        # Input shape\n",
    "        self.model_input = self.generator.input\n",
    "\n",
    "        # The generator model_input and generates img\n",
    "        img = self.generator(self.model_input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes the generated images as input and determines validity\n",
    "        validity = self.discriminator([img, self.model_input])\n",
    "\n",
    "        # The assembled model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.assemble = models.Model(self.model_input, [validity, img])\n",
    "        self.assemble.compile(\n",
    "            loss=assemble_loss,\n",
    "            optimizer=assemble_optimizer,\n",
    "            loss_weights=assemble_loss_weights,\n",
    "        )\n",
    "\n",
    "        super().__init__(self.generator, **kwargs)\n",
    "\n",
    "    def fit(self, data_generator, epochs, steps_per_epoch=None, **kwargs):\n",
    "        for key in kwargs.keys():\n",
    "            warnings.warn(\n",
    "                \"{0} not implemented for cgan. Does not affect the execution.\".format(\n",
    "                    key\n",
    "                )\n",
    "            )\n",
    "        history = np.zeros((4,epochs))\n",
    "        for epoch in range(epochs):\n",
    "            steps = steps_per_epoch\n",
    "\n",
    "            d_loss = 0\n",
    "            g_loss = 0\n",
    "\n",
    "            for step in range(steps):\n",
    "                ## update data\n",
    "                try:\n",
    "                    data, labels = next(data_generator)\n",
    "                except:\n",
    "                    data, labels = data_generator[step]\n",
    "\n",
    "                # Grab disriminator labels\n",
    "                shape = (data.shape[0], *self.discriminator.output.shape[1:])\n",
    "                valid, fake = np.ones(shape), np.zeros(shape)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator(data)\n",
    "                #gen_imgs[gen_imgs<0] = 0\n",
    "                \n",
    "                d_loss_real = self.discriminator.train_on_batch([labels[...,0:1], data], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, data], fake)\n",
    "                # make train on batch on \n",
    "                d_loss += 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (to have the discriminator label samples as valid)\n",
    "                train_assembler = 1\n",
    "                if train_assembler:\n",
    "                    g_loss += np.array(self.assemble.train_on_batch(data, [valid, labels]))\n",
    "\n",
    "                # Plot the progress\n",
    "\n",
    "            try:\n",
    "                data_generator.on_epoch_end()\n",
    "            except:\n",
    "                pass\n",
    "            history[0,epoch] = d_loss[0]\n",
    "            history[1,epoch] = g_loss[0]\n",
    "            history[2,epoch] = g_loss[1]\n",
    "            history[3,epoch] = g_loss[2]\n",
    "            if train_assembler:\n",
    "                print(\n",
    "                    \"%d [D loss: %f, acc.: %.2f%%] [G loss: %f, %f, %f]\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        d_loss[0] / steps,\n",
    "                        100 * d_loss[1] / steps,\n",
    "                        g_loss[0] / steps,\n",
    "                        g_loss[1] / steps,\n",
    "                        g_loss[2] / steps,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"%d [D loss: %f, acc.: %.2f%%]\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        d_loss[0] / steps,\n",
    "                        100 * d_loss[1] / steps,\n",
    "                    )\n",
    "                )\n",
    "        return history\n",
    "    \n",
    "\n",
    "class DualDiscriminatorGAN(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator=None,\n",
    "        discriminator=None,\n",
    "        discriminator_loss=None,\n",
    "        discriminator_optimizer=None,\n",
    "        discriminator_metrics=None,\n",
    "        assemble_loss=None,\n",
    "        assemble_optimizer=None,\n",
    "        assemble_loss_weights=None,\n",
    "        upper_threshold=0.95,\n",
    "        lower_threshold=0.51,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.upper_threshold=upper_threshold\n",
    "        self.lower_threshold=lower_threshold\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = discriminator\n",
    "        for i in range(len(discriminator)):\n",
    "            self.discriminator[i].compile(\n",
    "                loss=discriminator_loss[i],\n",
    "                optimizer=discriminator_optimizer[i],\n",
    "                metrics=discriminator_metrics[i],\n",
    "            )\n",
    "        \n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = generator\n",
    "\n",
    "        # Input shape\n",
    "        self.model_input = self.generator.input\n",
    "        \n",
    "        self.condition = tf.keras.layers.Input(shape=self.generator.output.shape[1:])\n",
    "        # The generator model_input and generates img\n",
    "        img = self.generator(self.model_input)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        for i in range(len(discriminator)):\n",
    "            self.discriminator[i].trainable = False\n",
    "        \n",
    "\n",
    "        # The discriminator takes the generated images as input and determines validity\n",
    "        validity = [self.discriminator[i](tf.keras.layers.Concatenate()([img, self.condition])) for i in range(len(discriminator))]\n",
    "        assembleoutput=validity\n",
    "        assembleoutput.append(img)\n",
    "        # The assembled model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.assemble = models.Model([self.model_input,self.condition], assembleoutput)\n",
    "        self.assemble.compile(\n",
    "            loss=assemble_loss,\n",
    "            optimizer=assemble_optimizer,\n",
    "            loss_weights=assemble_loss_weights,\n",
    "        )\n",
    "\n",
    "        super().__init__(self.generator, **kwargs)\n",
    "\n",
    "    def fit(self, data_generator, epochs, steps_per_epoch=None, **kwargs):\n",
    "        for key in kwargs.keys():\n",
    "            warnings.warn(\n",
    "                \"{0} not implemented for cgan. Does not affect the execution.\".format(\n",
    "                    key\n",
    "                )\n",
    "            )\n",
    "        train_discriminator=np.ones((len(self.discriminator)))\n",
    "        history = np.zeros((4,epochs))\n",
    "        for epoch in range(epochs):\n",
    "            steps = steps_per_epoch\n",
    "\n",
    "            d_loss = 0\n",
    "            \n",
    "            g_loss = 0            \n",
    "            \n",
    "            for step in range(steps):\n",
    "                d_loss_all=[]\n",
    "                ## update data\n",
    "                try:\n",
    "                    data, labels = next(data_generator)\n",
    "                except:\n",
    "                    data, labels = data_generator[step]\n",
    "                train_assembler = 1\n",
    "\n",
    "                gen_imgs = self.generator(data)\n",
    "\n",
    "                all_data=np.zeros((2*gen_imgs.shape[0], *gen_imgs.shape[1:]))\n",
    "                all_data[:data.shape[0]]=labels[:,0]\n",
    "                all_data[data.shape[0]:]=gen_imgs\n",
    "                all_data_2=np.zeros((2*gen_imgs.shape[0], *gen_imgs.shape[1:]))\n",
    "                all_data_2[:data.shape[0]]=labels[:,1]#data\n",
    "                all_data_2[data.shape[0]:]=labels[:,1]#data\n",
    "                \n",
    "                discriminator_input=np.concatenate((all_data,all_data_2),axis=-1)\n",
    "                \n",
    "                # Grab disriminator labels\n",
    "                vtot=[]\n",
    "                for i in range(len(self.discriminator)):\n",
    "                    shape = (data.shape[0], *self.discriminator[i].output.shape[1:])\n",
    "                    valid, fake = np.ones(shape), np.zeros(shape)\n",
    "                    vtot.append(valid)\n",
    "                    validfake=np.ones((2*data.shape[0], *self.discriminator[i].output.shape[1:]))\n",
    "                    validfake[:data.shape[0]]=valid\n",
    "                    validfake[data.shape[0]:]=fake\n",
    "                    if not train_discriminator[i]:\n",
    "                        d_loss_all.append(self.discriminator[i].train_on_batch(np.concatenate((all_data,all_data_2),axis=-1),validfake))\n",
    "                        if d_loss_all[i][1]<self.upper_threshold:\n",
    "                            train_discriminator[i]=1\n",
    "                    else:\n",
    "                        d_loss_all.append(self.discriminator[i].train_on_batch(np.concatenate((all_data,all_data_2),axis=-1),validfake))\n",
    "                        if d_loss_all[i][1]>self.upper_threshold:\n",
    "                            train_discriminator[i]=0\n",
    "                    \n",
    "                if np.max(np.array(d_loss_all)[:,1])>self.lower_threshold:\n",
    "                    train_assembler=0\n",
    "                if train_assembler:\n",
    "                    vtot.append(labels[:,0])\n",
    "                    g_loss += np.array(self.assemble.train_on_batch([data,labels[:,1]], vtot))\n",
    "                    \n",
    "                d_loss=np.mean(np.array(d_loss_all),axis=0)\n",
    "\n",
    "            try:\n",
    "                data_generator.on_epoch_end()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            history[0,epoch] = d_loss[0]\n",
    "            try:\n",
    "                history[1,epoch] = g_loss[0]\n",
    "                history[2,epoch] = g_loss[1]\n",
    "                history[3,epoch] = g_loss[2]\n",
    "            except:\n",
    "                history[1:,epoch] = 0\n",
    "            if train_assembler:\n",
    "                print(\n",
    "                    \"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        d_loss[0] / steps,\n",
    "                        100 * d_loss[1] / steps,\n",
    "                        g_loss[0] / steps\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"%d [D loss: %f, acc.: %.2f%%]\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        d_loss[0] / steps,\n",
    "                        100 * d_loss[1] / steps,\n",
    "                        \n",
    "                    \n",
    "                    )\n",
    "                )\n",
    "        return history\n",
    "    \n",
    "def reset_model(d_lr, g_lr, d_loss, g_loss, g_loss_weights):\n",
    "    discriminator, generator = init_model()\n",
    "    model = cgan(generator = generator, \n",
    "        discriminator = discriminator,\n",
    "        discriminator_loss = d_loss,\n",
    "        discriminator_optimizer = Adam(lr = d_lr, beta_1 = 0.5),\n",
    "        discriminator_metrics = \"accuracy\",\n",
    "        assemble_loss = g_loss,\n",
    "        assemble_optimizer = Adam(lr = g_lr, beta_1 = 0.5), \n",
    "        assemble_loss_weights = g_loss_weights,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Losses\n",
    "\n",
    "\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def unet_crossentropy(T, P):\n",
    "    weight1=K.sum(K.flatten(T))\n",
    "    weight2=K.sum(K.flatten(1-P))\n",
    "\n",
    "    weightT=weight1+weight2\n",
    "    weight1/=weightT\n",
    "    weight2/=weightT\n",
    "    eps = 1e-3\n",
    "    \n",
    "    weight1 = 1\n",
    "    weight2 = 1\n",
    "    return -K.mean(\n",
    "        weight1 * T * K.log(P + eps) + weight2 * (1 - T) * K.log(1 - P + eps)\n",
    "    )\n",
    "\n",
    "    \n",
    "from tensorflow.keras.optimizers import Adam    \n",
    "\n",
    "def mae_loss(T,P):\n",
    "    T_segment = T[...,0]\n",
    "    loss = mae(T_segment,P)\n",
    "    return loss\n",
    "\n",
    "def combined_loss(T,P):\n",
    "    T_segment = T[...,0]\n",
    "    D_true = T[:,0,0,1]\n",
    "    loss = mae(T_segment,P)\n",
    "    \n",
    "    traj_loss = mae(T,tf.math.multiply(T,P))\n",
    "    return K.sqrt(K.sqrt(loss))\n",
    "\n",
    "def mae_crossentropy_loss(T,P):\n",
    "    loss1 = unet_crossentropy(T,P)\n",
    "    loss2 = K.sqrt(mae(T,P))\n",
    "    loss3 = mae(T,tf.math.multiply(T,P))\n",
    "    return loss1 + 0.5*loss2 #+ 1/(1e-3 + loss3)\n",
    "\n",
    "def convloss(T,P):\n",
    "    return K.sqrt(K.sqrt(mae(T,P)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ParticleGenerator import GenNoise, Trajectory, init_particle_counter,input_array, PostProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure\n",
    "def batch_function(image):\n",
    "    img = image[...,:1]\n",
    "    img = skimage.measure.block_reduce(img,(T_reduction_factor,L_reduction_factor,1),np.mean)\n",
    "    return img\n",
    "\n",
    "def label_function(image):\n",
    "    img = image[...,1:2]\n",
    "    img = skimage.measure.block_reduce(img,(T_reduction_factor,L_reduction_factor,1),np.mean)\n",
    "    return img\n",
    "    \n",
    "def generate_training_batch(image,batch_size):\n",
    "    \n",
    "    min_data_size = batch_size\n",
    "    max_data_size = batch_size+1\n",
    "\n",
    "    data_generator=dt.generators.ContinuousGenerator(image,\n",
    "                                                batch_function=batch_function,\n",
    "                                                label_function=label_function,\n",
    "                                                batch_size=batch_size,\n",
    "                                                min_data_size=min_data_size,\n",
    "                                                max_data_size=max_data_size)\n",
    "\n",
    "    total_nr_epochs = 0\n",
    "    with data_generator:\n",
    "        GAN.fit(data_generator, epochs=0, steps_per_epoch=1)\n",
    "        \n",
    "    b,L = data_generator[0] \n",
    "    return b,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train cGAN\n",
    "\n",
    "import datetime\n",
    "\n",
    "TRAIN = 1\n",
    "lowiOC=1\n",
    "weightedLoss=1\n",
    "reduceInt = 0\n",
    "increaseDiff = 0\n",
    "use_val_data = 0\n",
    "\n",
    "batch_size = 4\n",
    "min_data_size = 16\n",
    "max_data_size = 32\n",
    "\n",
    "DEBUG = 0\n",
    "if DEBUG:\n",
    "    batch_size = 1\n",
    "    min_data_size = 1\n",
    "    max_data_size = 2\n",
    "\n",
    "reset_GAN =1\n",
    "load_GAN = 1\n",
    "save_GAN = 0\n",
    "val_freq_GAN = 20\n",
    "\n",
    "GAN_loadname='Network-weights/GAN-D0.1-2I0.0-1.0loss=0.0030516684.h5'\n",
    "\n",
    "load_unet = 1\n",
    "load_discriminator = 0\n",
    "save_unet = 0\n",
    "\n",
    "unet_loadname='Network-weights/U-net-D0.1-2I0.0-1.0loss=0.0030516684.h5'\n",
    "\n",
    "pathToEmpty=None\n",
    "steps_per_epoch = 1\n",
    "nbr_GAN_loops = 50000\n",
    "nbr_GAN_epochs_per_loop = 500\n",
    "\n",
    "length = 128*4\n",
    "L_reduction_factor = 4\n",
    "reduced_length = int(length/L_reduction_factor)\n",
    "\n",
    "times = 2048\n",
    "T_reduction_factor = 1\n",
    "reduced_times = int(times/T_reduction_factor)\n",
    "\n",
    "    \n",
    "max_nbr_particles = 8\n",
    "nump = lambda: 1+np.random.randint(3)\n",
    "\n",
    "Int = [5,6]\n",
    "D1 = 0.1 #20\n",
    "D2 = 1.2 #80\n",
    "\n",
    "Ds=[D1,D2]\n",
    "st = [0.04,0.05]   \n",
    "vel=lambda: (50000*np.random.rand())*10**-6\n",
    "\n",
    "getTrainTraj = Trajectory(intensity=Int,s=st,diffusion=Ds)\n",
    "\n",
    "#Normal image\n",
    "image=dt.FlipLR(dt.FlipUD(input_array(times=times,length=length) + init_particle_counter() \n",
    "                        + GenNoise(dX=lambda:.00001+.00003*np.random.rand(),\n",
    "                                                dA=0,#lambda:0+np.random.rand()*0.0001,\n",
    "                                                noise_lev=lambda:.0001,\n",
    "                                                biglam=lambda:0.6+.4*np.random.rand(),\n",
    "                                                bgnoiseCval=lambda:0.03+.02*np.random.rand(),\n",
    "                                                bgnoise=lambda:.08+.04*np.random.rand(),\n",
    "                                                bigx0=lambda: .1*np.random.randn(),\n",
    "                                                sinus_noise_amplitude=lambda: np.random.rand(),\n",
    "                                                freq =lambda: np.random.rand()*np.pi)\n",
    "                                    + getTrainTraj**nump\n",
    "                                    + PostProcess()))\n",
    "\n",
    "\n",
    "\n",
    "if reset_GAN:\n",
    "    d_lr = 1e-5\n",
    "    g_lr = 1e-5\n",
    "    d_loss = convloss\n",
    "    d_loss_2 = \"mse\"\n",
    "    if weightedLoss:\n",
    "        g_loss = [\"mae\",weighted_crossentropy(10,1)]\n",
    "    g_loss = [\"mae\",mae_crossentropy_loss],\n",
    "    g_loss_weights = [1,1]#,1]\n",
    "    GAN = reset_model(d_lr, g_lr, d_loss, g_loss, g_loss_weights)\n",
    "    best_model = reset_model(d_lr, g_lr, d_loss, g_loss, g_loss_weights)\n",
    "    best_net = '../input/gan-weights-11-nov-1115h5/weights_11_nov_1115.h5'\n",
    "if load_GAN:\n",
    "    GAN.load_weights(GAN_loadname)\n",
    "    best_model.load_weights(GAN_loadname)\n",
    "if load_unet:\n",
    "    GAN.generator.load_weights(unet_loadname)\n",
    "    best_model.generator.load_weights(unet_loadname)\n",
    "if load_discriminator:\n",
    "    GAN.discriminator.load_weights(disc_loadname)\n",
    "        \n",
    "### --- Data generator --- ###\n",
    "data_generator=dt.generators.ContinuousGenerator(image,\n",
    "                                            batch_function=batch_function,\n",
    "                                            label_function=label_function,\n",
    "                                            batch_size=batch_size,\n",
    "                                            min_data_size=min_data_size,\n",
    "                                            max_data_size=max_data_size)\n",
    "\n",
    "def GenerateValData(pathToEmpty=None):\n",
    "    # Set the paths for the data and the ground truth\n",
    "    if lowiOC:\n",
    "        val_path='../Data/Preprocessed chBInsulin 2x1 Simulated Data/lowiOC/diffusion/'\n",
    "        traj_path='../Data/Preprocessed chBInsulin 2x1 Simulated Data Ground Truth/lowiOC/'\n",
    "        val_path='../Data/Preprocessed DNA lowiOC Simulated Data/lowiOC/diffusion/'\n",
    "        traj_path='../Data/Preprocessed DNA lowiOC Simulated Data Ground Truth/lowiOC/'\n",
    "        val_path='../Data/Preprocessed lowiOC Simulated Data/lowiOC/diffusion/'\n",
    "        traj_path = '../Data/Preprocessed lowiOC Simulated Data Ground Truth/'\n",
    "    else:\n",
    "        # All IOC data\n",
    "        val_path='../Data/Preprocessed simulated data/allIOC/diffusion/'\n",
    "        traj_path = '../Data/Simulated Data - Ground Truth/'\n",
    "    \n",
    "    # Load the validation data\n",
    "    valFiles = os.listdir(val_path)\n",
    "    nrOFValFiles = int(len(valFiles)) \n",
    "    testFile=np.load(val_path+valFiles[0])\n",
    "    T = int(8192 / T_reduction_factor)  # Number of time points after reduction\n",
    "    L = int(512 / L_reduction_factor)   # Length of kymograph after reduction\n",
    "    TDiff = int((10000 - 8192) / T_reduction_factor / 2)  # Time crop\n",
    "    LDiff = int((600 - 512) / L_reduction_factor / 2)  # Spatial crop\n",
    "\n",
    "    valImgs = None  # Initialize validation data\n",
    "    valLabels = None  # Initialize validation labels\n",
    "    for i in range(nrOFValFiles):\n",
    "        # Read the intensity of the current file\n",
    "        intensity = float(valFiles[i][valFiles[i].index(\"iOC\")+3:valFiles[i].index(\"_M\")])*10000\n",
    "        \n",
    "        # Check if the intensity is above the minimum threshold\n",
    "        if intensity >= Int[0]:\n",
    "            # Load the current kymograph\n",
    "            file = np.expand_dims(np.load(val_path + valFiles[i]), -1)\n",
    "            \n",
    "            # Crop the kymograph in time and space and reduce the size\n",
    "            kymo = file[TDiff:T+TDiff, LDiff:-LDiff, :]\n",
    "            kymo = np.expand_dims(kymo, 0)\n",
    "            if valImgs is None:\n",
    "                valImgs = kymo\n",
    "            else:\n",
    "                valImgs = np.append(valImgs, kymo, 0)\n",
    "                \n",
    "            # Load the ground truth trajectory for the kymograph\n",
    "            try:\n",
    "                traj = np.load(traj_path + valFiles[i])\n",
    "            except:\n",
    "                trajFile = \"simulated\" + valFiles[i][valFiles[i].index(\"_\"):]\n",
    "                traj = np.load(traj_path + trajFile)\n",
    "                \n",
    "            # Transpose the trajectory if it has more columns than rows\n",
    "            if traj.shape[1] > traj.shape[0]:\n",
    "                traj = np.transpose(traj)\n",
    "            \n",
    "            # Reduce the size of the ground truth trajectory and normalize it\n",
    "            label = skimage.measure.block_reduce(traj, (T_reduction_factor, L_reduction_factor), np.mean)\n",
    "            label=label-1\n",
    "            label = np.abs(label/np.min(label))\n",
    "            try:\n",
    "                valLabels = np.append(valLabels,np.expand_dims(label[TDiff:T+TDiff,LDiff:-LDiff,:],0),0)\n",
    "            except:\n",
    "                valLabels = np.expand_dims(label[TDiff:T+TDiff,LDiff:-LDiff,:],0)\n",
    "\n",
    "    keepIndices = np.sum(valImgs,(1,2,3)) #some kymographs are empty\n",
    "    keepIndices=np.where(keepIndices!=0)[0]\n",
    "    valImgs=valImgs[keepIndices]\n",
    "    valLabels=valLabels[keepIndices]\n",
    "    \n",
    "    labelMax=np.expand_dims(np.max(valLabels,1),1)\n",
    "    labelMax[labelMax==0]=1\n",
    "    valLabels=valLabels/labelMax\n",
    "    \n",
    "    if pathToEmpty != None:\n",
    "        pathToEmpty= pathToEmpty+\"/diffusion/\"\n",
    "        emptyFiles=os.listdir(pathToEmpty)\n",
    "        emptyValImgs=np.array([np.load(pathToEmpty+emptyFiles[i]) for i in range(0,len(emptyFiles))])\n",
    "        emptyValImgs = emptyValImgs[:,TDiff:T+TDiff,LDiff:-LDiff]\n",
    "        emptyValImgs = np.expand_dims(emptyValImgs,-1)\n",
    "        emptyValLabels=np.zeros(emptyValImgs.shape)\n",
    "        \n",
    "        valImgs = np.append(valImgs,emptyValImgs,0)\n",
    "        valLabels = np.append(valLabels,emptyValLabels,0)\n",
    "\n",
    "    return valImgs,valLabels\n",
    "\n",
    "def GenerateTestData():\n",
    "    val_path = \"../Data/Preprocessed laserNoise Ferritin/ChannelE/\"\n",
    "    valFiles = os.listdir(val_path)\n",
    "    nrOFValFiles = int(3*len(valFiles)/3)\n",
    "    valImgs = np.zeros((nrOFValFiles,8192,128,1))\n",
    "    for i in range(0,nrOFValFiles):\n",
    "        file = np.expand_dims(np.load(val_path+valFiles[i]),-1) \n",
    "        valImgs[i,...]  = np.copy(file[904:8192+904,11:-11,:])\n",
    "        \n",
    "    return valImgs\n",
    "\n",
    "\n",
    "def GenerateValDataWithSize(valImgs,valLabels,size=512):\n",
    "    times = valImgs.shape[1]\n",
    "    randStart = np.random.randint(0,times-size)\n",
    "    newValImgs=valImgs[:,randStart:randStart+size,...]\n",
    "    newValLabels=valLabels[:,randStart:randStart+size,...]\n",
    "    \n",
    "    return newValImgs,newValLabels\n",
    "    \n",
    "\n",
    "### --- Init more params --- ###\n",
    "total_nr_epochs = 0\n",
    "MAX_NBR_EPOCHS = 500000\n",
    "meanLoss = np.inf\n",
    "try:\n",
    "    currentBestLoss\n",
    "except:\n",
    "    currentBestLoss = np.inf\n",
    "\n",
    "try:\n",
    "    meanLoss\n",
    "except:\n",
    "    meanLoss = [np.inf]*4\n",
    "\n",
    "save = True\n",
    "if use_val_data:\n",
    "    try: valImgs\n",
    "    except:\n",
    "            valImgs,valLabels = GenerateValData(pathToEmpty)\n",
    "        \n",
    "if load_unet:\n",
    "    try:\n",
    "       currentBestLoss =  float(unet_loadname[unet_loadname.index(\"loss\")+5:unet_loadname.index(\"loss\")+5+8])\n",
    "    except:\n",
    "        pass\n",
    "print(\"currentBestLoss= \"+str(currentBestLoss))\n",
    "### --- Training loop for cGAN--- ###\n",
    "if TRAIN:\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Iterate over data generator until maximum number of epochs is reached\n",
    "        with data_generator:\n",
    "            while total_nr_epochs < MAX_NBR_EPOCHS:\n",
    "                for ii in range(nbr_GAN_loops):\n",
    "                    # Train the GAN model for a certain number of epochs\n",
    "                    history = GAN.fit(data_generator, epochs=nbr_GAN_epochs_per_loop, steps_per_epoch=1)\n",
    "                    \n",
    "                    # Get the first batch and its loss\n",
    "                    b, L = data_generator[0]\n",
    "                    mean_loss = np.mean(np.mean(history, 1))\n",
    "                    \n",
    "                    # Adjust hyperparameters based on current loss\n",
    "                    if mean_loss < meanLoss and not use_val_data:\n",
    "                        if reduceInt:\n",
    "                            getTrainTraj.I[1] = np.max([0.95 * getTrainTraj.I[1], 0.8])\n",
    "                        meanLoss = np.mean(np.mean(history, 1))\n",
    "                    \n",
    "                    if increaseDiff:\n",
    "                        getTrainTraj.D[0] = np.min([1.01 * getTrainTraj.D[0], 1.25])\n",
    "                    \n",
    "                    # Use validation data\n",
    "                    if use_val_data:\n",
    "                        # Generate new validation data with a specific size\n",
    "                        newValImgs, newValLabels = GenerateValDataWithSize(valImgs, valLabels, size=1024)\n",
    "                        # Use the generator to predict the labels of the new validation data\n",
    "                        b = GAN.generator.predict(newValImgs[:, ...], batch_size=1)\n",
    "                        L = newValLabels[:, ...]\n",
    "                        # Calculate validation loss\n",
    "                        val_loss = mae(L, b) # change loss here\n",
    "                        print(\"valLoss = \" + str(val_loss))\n",
    "                        # Train the generator model with the new validation data\n",
    "                        GAN.generator.fit(newValImgs, newValLabels, batch_size=4, epochs=1)\n",
    "                        \n",
    "                        # Update hyperparameters based on current validation loss (curriculum learning)\n",
    "                        if val_loss < currentBestLoss:\n",
    "                            if reduceInt:\n",
    "                                currentInt = np.copy(getTrainTraj.I[1])\n",
    "                                getTrainTraj.I[1] = np.max([0.95 * getTrainTraj.I[1], 1])\n",
    "                                print(getTrainTraj.I[1])\n",
    "                            \n",
    "                            currentBestLoss = np.copy(val_loss)\n",
    "                            # Save the GAN and generator models\n",
    "                            GAN.save('GAN-' + \"D\" + str(round(D1, 2)) + \"-\" + str(round(D2, 2)) + \"I\" + str(round(getTrainTraj.I[0], 2)) + \"-\" + str(round(getTrainTraj.I[1], 2)) + \"loss=\" + str(val_loss.numpy()) + \"chA.h5\")\n",
    "                            GAN.generator.save('U-net-' + \"D\" + str(round(D1, 2)) + \"-\" + str(round(D2, 2)) + \"I\" + str(round(getTrainTraj.I[0], 2)) + \"-\" + str(round(getTrainTraj.I[1], 2)) + \"loss=\" + str(val_loss.numpy()) + \"chA.h5\")\n",
    "                    \n",
    "                    else:\n",
    "                        if save:\n",
    "                            # Save the GAN and generator models\n",
    "                            GAN.save('GAN-' + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\") + \" I\" + str(round(getTrainTraj.I[0], 2)) + \"-\" + str(round(getTrainTraj.I[1], 2)))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#%%\n",
    "plt.close('all')\n",
    "save=0\n",
    "savePath=\"../Figures/Meetings/2022-11-29/SimulatedPredictions/\"\n",
    "\n",
    "if save:\n",
    "    try:\n",
    "        os.makedirs(savePath)\n",
    "    except:\n",
    "        pass\n",
    "getTestTraj = Trajectory(I=[0,0],s=st,D=[1.2,2])\n",
    "#getTestTraj = Trajectory(I=[1,1.51],s=st)\n",
    "\n",
    "testImage=dt.FlipLR(dt.FlipUD(input_array() + init_particle_counter() \n",
    "                        + GenNoise(dX=lambda:.00001+.00003*np.random.rand(),\n",
    "                                                dA=0,\n",
    "                                                noise_lev=lambda:.0001,\n",
    "                                                biglam=lambda:0.6+.4*np.random.rand(),\n",
    "                                                bgnoiseCval=lambda:0.03+.02*np.random.rand(),\n",
    "                                                bgnoise=lambda:.08+.04*np.random.rand(),\n",
    "                                                bigx0=lambda: .1*np.random.randn(),\n",
    "                                                sinus_noise_amplitude=lambda: np.random.rand(),\n",
    "                                                freq =lambda: np.random.rand()*np.pi)\n",
    "                                  + getTrainTraj**nump\n",
    "                                  + PostProcess()))\n",
    "\n",
    "# image=dt.FlipLR(dt.FlipUD(input_array() + init_particle_counter() \n",
    "#                         + GenNoise(dX=lambda:.00001+.00003*np.random.rand(),\n",
    "#                                                 dA=lambda:0+np.random.rand()*0.0001,\n",
    "#                                                 noise_lev=lambda:.0001,\n",
    "#                                                 biglam=lambda:0.6+.4*np.random.rand(),\n",
    "#                                                 bgnoiseCval=lambda:0.03+.02*np.random.rand(),\n",
    "#                                                 bgnoise=lambda:.08+.04*np.random.rand(),\n",
    "#                                                 bigx0=lambda: .1*np.random.randn(),\n",
    "#                                                 sinus_noise_amplitude=lambda: np.random.rand(),\n",
    "#                                                 freq =lambda: np.random.rand()*np.pi)\n",
    "#                                     + getTrainTraj**nump\n",
    "#                                     + PostProcess()))\n",
    "\n",
    "\n",
    "# testImage=dt.FlipLR(dt.FlipUD(input_array() + init_particle_counter() \n",
    "#                         + GenNoise(dX=.0001,#+.00003*np.random.rand(),\n",
    "#                                     dA=0,\n",
    "#                                     noise_lev=.0002*(0.5+0.5*np.random.rand()),#+0.00005*np.random.randn(),\n",
    "#                                     biglam=0.5+.7*np.random.rand(),\n",
    "#                                     bgnoiseCval=0.05*np.random.rand(),\n",
    "#                                     bgnoise=.08+.04*np.random.rand(),\n",
    "#                                     bigx0=lambda: .1*np.random.randn(),\n",
    "#                                     sinus_noise_amplitude= np.random.rand(),\n",
    "#                                     freq = np.random.rand()*np.pi)#2*np.random.rand()))\n",
    "#                                     +getTestTraj**nump\n",
    "#                                     + PostProcess()))\n",
    "\n",
    "#newValImgs,newValLabels = valImgs,valLabels\n",
    "b,L = generate_training_batch(testImage,4)\n",
    "print(L.shape)\n",
    "for j in range(0,4):\n",
    "    fig, ax = plt.subplots(2,2,figsize=(16,16))      \n",
    "    \n",
    "    # Input image\n",
    "    img = b[j,...]\n",
    "    cax = ax[0][0]\n",
    "    cax.set_title('Kymograph',fontsize=16)\n",
    "    \n",
    "    cax.imshow(np.squeeze(img).T,aspect='auto')\n",
    "    \n",
    "    # Label image\n",
    "    cax = ax[1][0]\n",
    "    try:\n",
    "        label_img = np.reshape(L[j,...,0],(1,reduced_times,reduced_length,1))\n",
    "    except: \n",
    "        label_img = np.reshape(L[j,0,...,0],(1,reduced_times,reduced_length,1))\n",
    "        \n",
    "    cax.imshow(np.squeeze(label_img).T,aspect='auto')\n",
    "    cax.set_title('Ground truth',fontsize=16)\n",
    "    \n",
    "    # Predicted image current net\n",
    "    cax = ax[0][1]\n",
    "    pred_img = GAN.generator.predict(np.reshape(b[j,...],(1,reduced_times,reduced_length,1)))\n",
    "    #pred_img[pred_img < 0] = 0\n",
    "    im = cax.imshow(np.squeeze(pred_img).T,aspect='auto')\n",
    "    cax.set_title('Predicted Trajectory',fontsize=16)\n",
    "   # cax.plot(np.fft.fft2(img[:,256,0]))\n",
    "    \n",
    "    cax = ax[1][1]\n",
    "    cax.set_title('Prediction histogram',fontsize=16)\n",
    "    plt.hist(pred_img[pred_img>0.01].flatten(), density=True, bins=1000)\n",
    "    #plt.hist(b[j,:,:,0].flatten(), density=True, bins=1000)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(savePath+str(j)+\".png\")\n",
    "    \n",
    "    # plt.figure(figsize=(16,16))\n",
    "    # plt.imshow(np.squeeze(img).T,aspect='auto')\n",
    "    \n",
    "    i=np.random.randint(0,len(newValImgs))\n",
    "    fig, ax = plt.subplots(2,2,figsize=(16,16))      \n",
    "    \n",
    "    # Input image\n",
    "    img = newValImgs[i,...]\n",
    "    cax = ax[0][0]\n",
    "    cax.set_title('Kymograph',fontsize=16)\n",
    "    cax.set_xlabel(\"t (frames)\")\n",
    "    cax.set_ylabel(\"x (pixels/4)\")\n",
    "    \n",
    "    cax.imshow(np.squeeze(img).T,aspect='auto')\n",
    "    \n",
    "    # Label image\n",
    "    cax = ax[1][0]\n",
    "    try:\n",
    "        label_img = newValLabels[i,...,0]\n",
    "    except: \n",
    "        label_img = np.reshape(newValLabels[i,0,...,0],(1,reduced_times,reduced_length,1))\n",
    "        \n",
    "    cax.imshow(np.squeeze(label_img).T,aspect='auto')\n",
    "    cax.set_title('Segment label',fontsize=16)\n",
    "    cax.set_xlabel(\"t (frames)\")\n",
    "    cax.set_ylabel(\"x (pixels/4)\")\n",
    "    \n",
    "    cax=ax[1][0]\n",
    "    #cax.imshow(np.log(np.abs(np.fft.fftshift(np.fft.fft2(img[:,:,0])))))\n",
    "    # Predicted image current net\n",
    "    cax = ax[0][1]\n",
    "    pred_img = GAN.generator.predict(np.expand_dims(img,0))\n",
    "    #pred_img[pred_img < 0] = 0\n",
    "    im = cax.imshow(np.squeeze(pred_img).T,aspect='auto')\n",
    "    cax.set_xlabel(\"t (frames)\")\n",
    "    cax.set_ylabel(\"x (pixels/4)\")\n",
    "    \n",
    "    cax = ax[1][1]\n",
    "    plt.hist(pred_img[pred_img>0.01].flatten(), density=True, bins=1000)\n",
    "    #plt.hist(newValImgs[i,:,:,0].flatten(), density=True, bins=1000)#\n",
    "   # im = cax.imshow(np.squeeze(pred_img).T,aspect='auto')\n",
    "    cax.set_xlabel(\"t (frames)\")\n",
    "    cax.set_ylabel(\"x (pixels/4)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(savePath+str(j)+\"-val.png\")\n",
    "    # plt.figure(figsize=(16,16))\n",
    "    # plt.imshow(np.squeeze(img).T,aspect='auto')\n",
    "    \n",
    "#%% Study cross section of channels\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "path=r\"C:\\Users\\ccx55\\OneDrive\\Documents\\GitHub\\Phd\\Biosensing---nanochannel-project\\Data\\chBInsulin Simulated Data\\lowiOC\"\n",
    "path=\"../Data/2022-12-16/TBE/ch2B-4/\"\n",
    "\n",
    "#path=r\"C:\\Users\\ccx55\\OneDrive\\Documents\\GitHub\\Phd\\Biosensing---nanochannel-project\\Data\\2022-11-17-DNA\\TE-buff\\3C-2\"\n",
    "path = path.replace(os.sep, '/') + \"/\"\n",
    "import scipy.io as io\n",
    "frames=100\n",
    "files = os.listdir(path)\n",
    "file = files[0]\n",
    "\n",
    "img=io.loadmat(path+file)\n",
    "try:\n",
    "    img=img[\"data\"][\"Im\"][0][0]\n",
    "except:\n",
    "    img=img[\"Im\"].squeeze()\n",
    "plt.figure()\n",
    "for i in range(0,frames):\n",
    "    # img[i,:] -= np.min(img[i,:])\n",
    "    # img[i,:] =img[i,:]/np.max(img[i,:])\n",
    "    plt.plot(img[i,:])\n",
    "    \n",
    "path=r\"Z:\\NSM\\simulated_tests\\experimental_noise\\empty_channel_50x30\"\n",
    "path = path.replace(os.sep, '/') + \"/\"\n",
    "import scipy.io as io\n",
    "\n",
    "files = os.listdir(path)\n",
    "file = files[9]\n",
    "\n",
    "img=io.loadmat(path+file)\n",
    "img=img[\"data\"][\"Im\"][0][0]\n",
    "\n",
    "plt.figure()\n",
    "for i in range(0,frames):\n",
    "    img[i,:] -=np.min(img[i,:])\n",
    "    img[i,:] /=np.max(img[i,:])\n",
    "    plt.plot(img[i,:])\n",
    "#%%Test on experimental data - folder\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "maxFiles=20\n",
    "save=1\n",
    "plt.close('all')\n",
    "unet_path='U-net-D0.1-2I0.0-1.0loss=0.003131276.h5'\n",
    "unet_path='U-Net- I0.01-25_loss_0.009943331.h5'\n",
    "unet_path=\"U-net-D0.6-1.2I0.1-0.6loss=0.0050201593chA.h5\"\n",
    "#unet_path=\"U-net-D0.1-2I0.0-1.0loss=0.0058968016chA.h5\"\n",
    "#unet_path=\"U-net-D1-2I0.0-1.0loss=0.0044185026chA.h5\"\n",
    "unet_path=\"U-net-D0.1-1.2I0.1-1loss=0.0071075866chA.h5\" #old labelling scheme\n",
    "unet_path=\"U-net-D0.1-1.2I0.1-1loss=0.011916474chA.h5\" #trained on pathToEmpty files\n",
    "unet_path=\"U-net-D0.1-1.2I0.1-1loss=0.013097599chA.h5\" #works well for 200bps, lots of noise in new measurements though\n",
    "#unet_path=\"U-net-D0.1-1.2I0.01-0.8loss=0.0116292965chA.h5\" #trained with sometimes 0 particles in kymo\n",
    "unet =tf.keras.models.load_model(unet_path,compile=False)#GAN.generator\n",
    "\n",
    "#exp_path=\"../Data/Preproces\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-10-21-DNA\\50bp\"\n",
    "exp_path=r\"..\\Data\\2022-01-Inulin\\Insulin\\ML\"\n",
    "exp_path=r\"..\\Data\\Preprocessed DNA lowiOC Simulated Data\"\n",
    "#exp_path=r\"..\\Data\\Preprocessed 2022-11-17-DNA\\TE-buff\" \n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-11-17-DNA\\200bps-9.5-molperch\"\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-12-06\\200bp-3-p-ch-20vpp-300hz\"\n",
    "exp_path=r\"../Data/Preprocessed 2022-12-06/TE-buffer/\"\n",
    "exp_path=r\"../Data/Preprocessed 2022-12-06/100bp-3-p-ch/\"\n",
    "exp_path=r\"../Data/Preprocessed 2022-12-07/200bp-3-p-ch/\"\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-12-16\\TBE\"\n",
    "\n",
    "#exp_path=r\"..\\Data\\Preprocessed 2022-11-30-cont-test\\TE-buffer\"\n",
    "#exp_path=r\"..\\Data\\Preprocessed 2022-09-16-noise-10-hours\"\n",
    "#exp_path=r\"..\\Data\\Preprocessed 2022-09-15-noise-10-hours\"\n",
    "# exp_path=r\"..\\Data\\Preprocessed 2022-07-26-200bps-0.1M-salt\"\n",
    "# exp_path=r\"..\\Data\\Preprocessed 13-09-2022-3C-3-noise-measure\"\n",
    "# exp_path=r\"..\\Data\\Preprocessed 100bps-0.1salt-TE\"\n",
    "# exp_path=r\"..\\Data\\Preprocessed 100bps-0.1salt-TE\"\n",
    "# exp_path=r\"..\\Data\\Preprocessed data-david\"\n",
    "#exp_path=r\"..\\Data\\Preprocessed DNA lowiOC Simulated Data\"\n",
    "#exp_path=r\"..\\Data\\Preprocessed simulated data\"\n",
    "#exp_path=r\"..\\Data\\Preprocessed Old Experimental Noise\"\n",
    "\n",
    "#fix path on windows\n",
    "exp_path = exp_path.replace(os.sep, '/') + \"/\"\n",
    "\n",
    "savePath=\"../Figures/Meetings/2022-12-16/Noise/TBE/ch2B-2\"\n",
    "\n",
    "if save:\n",
    "    try:\n",
    "        os.makedirs(savePath)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "folders=os.listdir(exp_path)\n",
    "counter=0\n",
    "for folder in folders:\n",
    "    folderName=folder+\"/\"\n",
    "    # if \"B\" in folder: #skip channel B, too high vibrations\n",
    "    #     continue\n",
    "    folder = folder+\"/diffusion/\"\n",
    "    files = os.listdir(exp_path+folder)\n",
    "    counter = 0\n",
    "    if save:\n",
    "        try:\n",
    "            os.makedirs(savePath+folderName)\n",
    "        except:\n",
    "            pass\n",
    "    for file in files:\n",
    "        fileName=file\n",
    "\n",
    "        fileName=fileName.replace(\".mat\",\"\")\n",
    "        fileName=fileName.replace(\".npy\",\"\")\n",
    "        fileName=fileName.replace(\"iOC0.\",\"iOC0\")\n",
    "        file = exp_path+folder+file\n",
    "        fig, axs = plt.subplots(2,2,figsize=(16,16))      \n",
    "        axs=axs.flatten()\n",
    "        counter+=1\n",
    "       # if \"0.0001\" not in file:\n",
    "        #    continue\n",
    "        data = np.load(file)\n",
    "        data = np.expand_dims(data,(0,-1))\n",
    "        try:\n",
    "            orig_img = data[:,200:-200,11:-11,:]\n",
    "            pred=unet.predict(orig_img)\n",
    "            img=axs[0].imshow(data[0,200:-200,11:-11,0],aspect='auto')  \n",
    "        except:\n",
    "            orig_img = data[:,199:-200,11:-11,:]\n",
    "            pred=unet.predict(orig_img)\n",
    "            img=axs[0].imshow(data[0,199:-200,11:-11,0],aspect='auto')\n",
    "        #pred[pred>0.01]=1\n",
    "        plt.suptitle(exp_path[8:]+fileName)\n",
    "        plt.colorbar(img,ax=axs[0])\n",
    "\n",
    "        #plt.savefig(\"../Figures/Meetings/\"+file+\"-kymograph\")\n",
    "        timesLimit = data.shape[1] % 128\n",
    "        times = data.shape[1]\n",
    "        # if timesLimit > 0:\n",
    "        #    # orig_img = data[:,0:-int(timesLimit),11:139,:]\n",
    "        #    orig_img = data[:,0:-int(timesLimit),11:-11,:]\n",
    "      #  else:\n",
    "        \n",
    "           # orig_img = data[:,:,11:139,:]\n",
    "        #orig_img[:,:,0:15,:] = 0\n",
    "        #orig_img[:,:,110:,:] = 0\n",
    "    \n",
    "            \n",
    "    \n",
    "        axs[1].imshow(pred[0,:,:,0],aspect='auto')\n",
    "        \n",
    "\n",
    "        axs[2].hist(orig_img.flatten(), density=True, bins=1000)\n",
    "        axs[3].hist(pred[pred>0.01].flatten(), density=True, bins=100)\n",
    "        if save:\n",
    "            plt.savefig(savePath+folderName+fileName)\n",
    "            # np.savetxt(savePath+file+\"-kymograph\",orig_img[0,:,:,0].flatten())\n",
    "            # np.savetxt(savePath+file+\"-probMap\",pred[0,:,:,0].flatten())\n",
    "            plt.close('all')\n",
    "        counter+=1\n",
    "        if counter>maxFiles:\n",
    "            break\n",
    "        \n",
    "    if counter>maxFiles:\n",
    "        break\n",
    "\n",
    "#%%WeightToIOC\n",
    "import numpy as np\n",
    "\n",
    "weight = 132e3/4 #50 bp DNA\n",
    "A=50*50*1e-6 #ChE new chip\n",
    "\n",
    "weight = [132e3,132e3/2,132e3/4] #200,100,50 bp DNA\n",
    "A=115*50*1e-6 #ChB old chip\n",
    "A=(114+130)/2*97*1e-6 #ChC old chip\n",
    "A=(130)*97*1e-6 #ChC old chip\n",
    "#weight = [5.6e3] #insulin\n",
    "A=47*68*1e-6 #50x50 channel\n",
    "A=20*50*1e-6 #smallest channels available, roughly\n",
    "#A=100*27*1e-6 #BSA measured channel\n",
    "\n",
    "\n",
    "#weight = 66e3 #BSA\n",
    "#A=27*100*1e-6 #BSA channel\n",
    "\n",
    "n_i=1.33\n",
    "n_o=1.46\n",
    "\n",
    "n_TE = 2*n_i**2/(n_i**2 - n_o**2)\n",
    "n_TM = (n_i**2 + n_o**2)/(n_i**2 - n_o**2)\n",
    "n_mean = 0.5*(n_TE + n_TM)\n",
    "alpha_MW = 0.461e-12\n",
    "\n",
    "calibration = A/alpha_MW/n_mean\n",
    "iOC = [w/calibration for w in weight]\n",
    "print(np.array(iOC)*10000)\n",
    "\n",
    "#%%WeightToIOC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "A=50*50*1e-6 #ChE new chip\n",
    "\n",
    "iOC=[0.125] #times 10, my units\n",
    "A=115*50*1e-6 #ChB old chip\n",
    "A=(114+130)/2*97*1e-6 #ChC old chip\n",
    "A=(130)*97*1e-6 #ChC old chip\n",
    "#weight = [5.6e3] #insulin\n",
    "A=47*68*1e-6 #50x50 channel\n",
    "A=20*50*1e-6 #smallest channels available, roughly\n",
    "#A=100*27*1e-6 #BSA measured channel\n",
    "\n",
    "\n",
    "#weight = 66e3 #BSA\n",
    "#A=27*100*1e-6 #BSA channel\n",
    "\n",
    "n_i=1.33\n",
    "n_o=1.46\n",
    "\n",
    "n_TE = 2*n_i**2/(n_i**2 - n_o**2)\n",
    "n_TM = (n_i**2 + n_o**2)/(n_i**2 - n_o**2)\n",
    "n_mean = 0.5*(n_TE + n_TM)\n",
    "alpha_MW = 0.461e-12\n",
    "\n",
    "calibration = A/alpha_MW/n_mean\n",
    "w = [iOC*calibration for iOC in iOC]\n",
    "print(np.array(iOC)*10000)\n",
    "\n",
    "\n",
    "#%%Test on experimental data\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "save=1\n",
    "maxNrFiles=200\n",
    "plt.close('all')\n",
    "unet_path = 'unet-14-dec-1700.h5'\n",
    "unet_path=\"U-Net- I0.01-25_loss_0.009943331.h5\"\n",
    "unet_path=\"U-net-D0.1-2I0.0-1.46loss=0.0074624214.h5\"\n",
    "unet_path=\"U-net-D0.1-2I0.0-1.0loss=0.0060592867.h5\"\n",
    "unet_path=\"U-net-D0.1-2I0.0-1.0loss=0.006284812.h5\"\n",
    "unet_path=\"U-net-D0.1-2I0.0-1.0loss=0.003131276.h5\"\n",
    "#unet_path=\"U-net-D0.1-2I0.0-1.0loss=0.0060042166chA.h5\"\n",
    "unet_path=\"U-net-D0.1-2I0.0-1.0loss=0.0058968016chA.h5\"\n",
    "unet = tf.keras.models.load_model(unet_path,compile=False)\n",
    "\n",
    "#exp_path=\"../Data/Preprocessed data-bohdan/200bps CropMedian/4B-5/diffusion/\" #probably 0 traj..\n",
    "exp_path=\"../Data/Preprocessed data-bohdan/200bps CropMedian/4C-3/diffusion/\" #maybe one traj, probably 0..\n",
    "\n",
    "exp_path=\"../Data/Preprocessed data-bohdan/2022-09-20-noisetest-ladder/ch2B-5-series/diffusion/\"\n",
    "exp_path=\"../Data/Preprocessed lowiOC 20220923 Simulated Data/lowiOC/diffusion/\"\n",
    "exp_path=\"../Data/Preprocessed data-david/ch1-2C-7/diffusion/\"\n",
    "exp_path=\"../Data/Preprocessed lowiOC 20220928chA Simulated Data/lowiOC/diffusion/\"\n",
    "#exp_path=\"../Data/Preprocessed 2022-09-23-DNA-fragments/100bps-NaCl-0.05TBE/ch1-2A-4/diffusion/\"\n",
    "#exp_path=\"../Data/Preprocessed 2022-09-23-DNA-fragments/100bps-NaCl-0.05TBE/ch1-2B-5/diffusion/\"\n",
    "#exp_path=\"../Data/Preprocessed 2022-09-28-DNA-fragments/50bps-0.01TBE/ch1-3A-6/diffusion/\"\n",
    "exp_path=\"../Data/Preprocessed 2022-09-28-DNA-fragments/water/ch1-3A-6/diffusion/\"\n",
    "exp_path=\"../Data/Preprocessed 2022-09-30-DNA/50-bps-8-molec-in-vol-TE-TBE/1-ch5A-5/diffusion/\"\n",
    "exp_path=\"../Data/Preprocessed 2022-09-30-DNA/TE-buffer/1-ch3A-5/diffusion/\"\n",
    "exp_path=\"../Data/2022-01-Inulin/Insulin/ML/1-3E-4/diffusion/\"\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-11-17-DNA\\200bps-9.5-molperch\\3B-5\\diffusion\"\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-11-17-DNA\\TE-buff\\3B-5\\diffusion\"\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-11-17-DNA\\TE-buff\\3C-2\\diffusion\"\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-11-17-DNA\\200bps-9.5-molperch\\3C-2\\diffusion\"\n",
    "#exp_path=r\"..\\Data\\2022-01-Inulin\\Insulin\"\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-12-16\\50-DNA\\ch2B-2\\diffusion\" #very few, maybe 1-2 trajectories\n",
    "exp_path=r\"..\\Data\\Preprocessed 2022-12-16\\50-DNA\\ch2B-4\\diffusion\" #tons of trajectories, but also a lot of noise.\n",
    "#exp_path=r\"..\\Data\\Preprocessed 2022-12-16\\insulin\\ch2B-3\\diffusion\" #looks like some trajectories? or just noise?\n",
    "#exp_path=r\"..\\Data\\Preprocessed 2022-12-16\\TBE\\ch2B-2\\diffusion\"\n",
    "\n",
    "#fix path on windows\n",
    "exp_path = exp_path.replace(os.sep, '/') + \"/\"\n",
    "\n",
    "\n",
    "\n",
    "savePath=\"../Figures/Meetings/2022-12-19/50-DNA/ch2B-4/\"\n",
    "#savePath=\"Y:/13-09-2022-3C-3-noise-measure/\"\n",
    "#savePath=\"Y:/2022-noise-meas/2022-09-20-noisetest-ladder/ch2B-5-series-2/\"\n",
    "if save:\n",
    "    try:\n",
    "        os.makedirs(savePath)\n",
    "    except:\n",
    "        pass\n",
    "files = os.listdir(exp_path)[:maxNrFiles]\n",
    "counter = 0\n",
    "for file in files:\n",
    "    fig,axs=plt.subplots(1,2,figsize=(16,16))\n",
    "    counter+=1\n",
    "   # if \"0.0001\" not in file:\n",
    "    #    continue\n",
    "    data = np.load(exp_path+file)\n",
    "    data = np.expand_dims(data,(0,-1))\n",
    "    try:\n",
    "        orig_img = data[:,200:-200,11:-11,:]\n",
    "        pred=unet.predict(orig_img)\n",
    "        img=axs[0].imshow(data[0,200:-200,11:-11,0],aspect='auto')  \n",
    "    except:\n",
    "        orig_img = data[:,199:-200,11:-11,:]\n",
    "        pred=unet.predict(orig_img)\n",
    "        img=axs[0].imshow(data[0,199:-200,11:-11,0],aspect='auto')\n",
    "        \n",
    "    plt.colorbar(img,ax=axs[0])\n",
    "    file=file.replace(\".mat\",\"\")\n",
    "    file=file.replace(\".npy\",\"\")\n",
    "    file=file.replace(\"iOC0.\",\"iOC0\")\n",
    "    #plt.savefig(\"../Figures/Meetings/\"+file+\"-kymograph\")\n",
    "    timesLimit = data.shape[1] % 128\n",
    "    times = data.shape[1]\n",
    "    # if timesLimit > 0:\n",
    "    #    # orig_img = data[:,0:-int(timesLimit),11:139,:]\n",
    "    #    orig_img = data[:,0:-int(timesLimit),11:-11,:]\n",
    "  #  else:\n",
    "    \n",
    "       # orig_img = data[:,:,11:139,:]\n",
    "    #orig_img[:,:,0:15,:] = 0\n",
    "    #orig_img[:,:,110:,:] = 0\n",
    "\n",
    "        \n",
    "\n",
    "    axs[1].imshow(pred[0,:,:,0],aspect='auto')\n",
    "    plt.title(file)\n",
    "    if save:\n",
    "        plt.savefig(savePath+file)\n",
    "        # np.savetxt(savePath+file+\"-kymograph\",orig_img[0,:,:,0].flatten())\n",
    "        # np.savetxt(savePath+file+\"-probMap\",pred[0,:,:,0].flatten())\n",
    "        plt.close('all')\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "b,L = generate_training_batch(image,1)\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(32,6))      \n",
    "\n",
    "# Input image\n",
    "j = 0#np.random.randint(batch_size)\n",
    "img = b[j,...]\n",
    "cax = ax[0][0]\n",
    "cax.imshow(np.squeeze(img).T,aspect='auto')\n",
    "\n",
    "# Label image\n",
    "cax = ax[1][0]\n",
    "#try:\n",
    "#    label_img = np.reshape(L[j,...,0],(reduced_times,reduced_times))\n",
    "#except: \n",
    "#    label_img = np.reshape(L[j,0,...,0],(reduced_times,reduced_length))\n",
    "label_img=L\n",
    "cax.imshow(np.squeeze(label_img).T,aspect='auto')\n",
    "cax.set_title('Segment label',fontsize=16)\n",
    "\n",
    "# Predicted image current net\n",
    "cax = ax[0][1]\n",
    "pred_img = GAN.generator.predict(np.reshape(b[j,...],(1,reduced_times,reduced_length,1)))\n",
    "#pred_img[pred_img < 0] = 0\n",
    "im = cax.imshow(np.squeeze(pred_img).T,aspect='auto')\n",
    "     \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#%%\n",
    "plt.close('all')\n",
    "for i in range(0,3):\n",
    "    img,label = generate_training_batch(image,1) \n",
    "    fig,axs=plt.subplots(1,3)\n",
    "    axs[0].imshow(img[0,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[1].imshow(GAN.generator.predict(img)[0,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[2].imshow(label[0,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[0].grid(False)\n",
    "    axs[1].grid(False)\n",
    "    axs[2].grid(False)\n",
    "    # plt.figure(100+i)\n",
    "    # plt.imshow(img[0,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[0].set_title(\"Original Kymograph\")\n",
    "    axs[1].set_title(\"Network Segmentation\")\n",
    "    axs[2].set_title(\"True Trajectories\")\n",
    "    \n",
    "    fig,axs=plt.subplots(1,3)\n",
    "    j=np.random.randint(len(valImgs))\n",
    "    axs[0].imshow(valImgs[j,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[1].imshow(GAN.generator.predict(np.expand_dims(valImgs[j,...],0))[0,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[2].imshow(valLabels[j,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[0].grid(False)\n",
    "    axs[1].grid(False)\n",
    "    axs[2].grid(False)\n",
    "    # plt.figure(100+i+1)\n",
    "    # plt.imshow(valImgs[j,:,:,0],aspect='auto',cmap=\"viridis\")\n",
    "    axs[0].set_title(\"Original Kymograph\")\n",
    "    axs[1].set_title(\"Network Segmentation\")\n",
    "    axs[2].set_title(\"True Trajectories\")\n",
    "#%% Test\n",
    "for i in range(0,4):\n",
    "    testImgs = GenerateValData()[0]\n",
    "    fig,axs=plt.subplots(1,2)\n",
    "    j=np.random.randint(len(testImgs))\n",
    "    axs[0].imshow(testImgs[j,:,:,0],aspect='auto')\n",
    "    axs[1].imshow(GAN.generator.predict(np.expand_dims(testImgs[j,...],0))[0,:,:,0],aspect='auto')\n",
    "    axs[0].set_title(\"Original Kymograph\")\n",
    "    axs[1].set_title(\"Network Segmentation\")\n",
    "#%%\n",
    "val_path='../Data/Preprocessed lowiOC Simulated Data/lowiOC/diffusion/'\n",
    "valFiles = os.listdir(val_path)\n",
    "#Change this in GenerateValData\n",
    "#valFiles = [valFile for valFile in valFiles if \"iOC0.0001\" in valFile or \"e-05\" in valFile]# or \"iOC7.5e-05\" in valFile or \"iOC5e-05\" in valFile or \"iOC2.5e-05\" in valFile]\n",
    "#valFiles = [valFile for valFile in valFiles if \"e-06\" in valFile]\n",
    "plt.close('all')\n",
    "saveDir = \"../Figures/lowiOC/\"+unet_loadname[:-3] + \"/\"\n",
    "save=1\n",
    "for i in range(0,20):\n",
    "    # img,label = generate_training_batch(image,1) \n",
    "    # fig,axs=plt.subplots(1,3)\n",
    "    # axs[0].imshow(img[0,:,:,0],aspect='auto')\n",
    "    # axs[1].imshow(GAN.generator.predict(img)[0,:,:,0],aspect='auto')\n",
    "    # axs[2].imshow(label[0,:,:,0],aspect='auto')\n",
    "    \n",
    "    fig,axs=plt.subplots(1,3,sharey=True,figsize=(16,16))\n",
    "    j=np.random.randint(len(valImgs))\n",
    "    axs[0].imshow(newValImgs[j,:,:,0],aspect='auto')\n",
    "    axs[1].imshow(GAN.generator.predict(np.expand_dims(newValImgs[j,:,:,0],0))[0,:,:,0],aspect='auto')\n",
    "    axs[2].imshow(newValLabels[j,:,:,0],aspect='auto')\n",
    "    axs[0].set_title(\"Kymograph\")\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "    axs[2].set_title(\"True Trajectories\")\n",
    "    intensity = np.round(float(valFiles[j][valFiles[j].index(\"iOC\")+3:valFiles[j].index(\"_M\")])*10000,2)\n",
    "    kDa = str(np.round(66*intensity,2))\n",
    "    fig.suptitle(\"iOC = \"+str(intensity)+str(\", kDa = \"+kDa))\n",
    "    \n",
    "    saveName = saveDir+\"-iOC = \"+str(intensity)+\", kDa = \"+kDa+\"_\"+str(i)\n",
    "\n",
    "    #plt.savefig(\"test\"+str(i))\n",
    "    if save:\n",
    "        try:\n",
    "            plt.savefig(saveName+\".png\")\n",
    "        except:\n",
    "            os.mkdir(saveDir)\n",
    "            plt.savefig(saveName+\".png\")\n",
    "    \n",
    "#%%\n",
    "plt.close('all')\n",
    "for j in range(0,40):\n",
    "\n",
    "    fig,axs=plt.subplots(1,2)\n",
    "    #j=np.random.randint(len(valImgs))\n",
    "    axs[0].imshow(valImgs[j,:,:,0],aspect='auto')\n",
    "    axs[1].imshow(valLabels[j,:,:,0],aspect='auto')\n",
    "    \n",
    "#%% SNR analysis\n",
    "MW = [60,55,50,45,40,35,30,25,20,10,5,2,1]\n",
    "MW = [1000,1]\n",
    "# init\n",
    "Int = [0,0]\n",
    "Ds= 0.7\n",
    "st = 0.045\n",
    "getTrainTraj = Trajectory(I=Int,s=st)\n",
    "factor=1\n",
    "# image=dt.FlipLR(dt.FlipUD(input_array() + trainingDiffInt\n",
    "#                           + GenNoise(dX=(.00001+.00003*np.random.rand())/factor,\n",
    "#                                                   dA=0,\n",
    "#                                                   noise_lev=(.0001)/factor/2,\n",
    "#                                                   biglam=(0.6+.4*np.random.rand())/factor,\n",
    "#                                                   bgnoiseCval=(0.03+.02*np.random.rand())/factor,\n",
    "#                                                   bgnoise=(.08+.04*np.random.rand())/factor,\n",
    "#                                                   bigx0=lambda: (.1*np.random.randn())/factor)\n",
    "#                                       + trainingTraj**nbr_particles\n",
    "#                                       + PostProcess()))\n",
    "\n",
    "image=dt.FlipLR(dt.FlipUD(input_array() + init_particle_counter() \n",
    "                        + GenNoise(dX=.00001+.00003*np.random.rand(),\n",
    "                                                dA=0,\n",
    "                                                noise_lev=.0001/2,\n",
    "                                                biglam=0.6+.4*np.random.rand(),\n",
    "                                                bgnoiseCval=0.03+.02*np.random.rand(),\n",
    "                                                bgnoise=.08+.04*np.random.rand(),\n",
    "                                                bigx0=lambda: .1*np.random.randn())\n",
    "                                    + getTrainTraj\n",
    "                                    + PostProcess()))\n",
    "\n",
    "img=image.resolve()\n",
    "noiseStd = np.std(np.array(img[...,0]))\n",
    "signalMean=np.zeros((len(MW)))\n",
    "factor=1\n",
    "for i in range(0,len(MW)):\n",
    "\n",
    "    getTrainTraj.I[0] = 1.15/66*MW[i]/(0.045*np.sqrt(2*np.pi)*length/2*0.0295)\n",
    "    getTrainTraj.I[1] = 1.15/66*MW[i]/(0.045*np.sqrt(2*np.pi)*length/2*0.0295)\n",
    "    image=dt.FlipLR(dt.FlipUD(input_array() + init_particle_counter() \n",
    "                            + GenNoise(dX=.00001+.00003*np.random.rand(),\n",
    "                                                    dA=0,\n",
    "                                                    noise_lev=.0001/2,\n",
    "                                                    biglam=0.6+.4*np.random.rand(),\n",
    "                                                    bgnoiseCval=0.03+.02*np.random.rand(),\n",
    "                                                    bgnoise=.08+.04*np.random.rand(),\n",
    "                                                    bigx0=lambda: .1*np.random.randn())\n",
    "                                        + getTrainTraj\n",
    "                                        + PostProcess()))\n",
    "\n",
    "\n",
    "    img=image.resolve()\n",
    "    label=img[...,1]\n",
    "    label=label[label!=0]\n",
    "    signalMean[i]=np.mean(np.array(label))\n",
    "\n",
    "SNR=signalMean/noiseStd\n",
    "print(SNR)\n",
    "#%%\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.ticker as plticker\n",
    "import matplotlib.patches as patches\n",
    "def manTrack(diffImg,keepTraj=4,threshold=0.1,trajTreshold=64,dist=16):\n",
    "    #takes in an image\n",
    "    #keepTraj minimum length of accepted trajectory\n",
    "    #treshold minimum height of local maxima\n",
    "    #trajTreshold minimum distance from traj to point to consider point in traj\n",
    "    #dist minimum distance between trajectories\n",
    "    frames={}\n",
    "    flag=0\n",
    "    #Find local maxima in each frame\n",
    "    for f in range(0,diffImg.shape[0]):\n",
    "        frame = diffImg[f,:]\n",
    "        localMax = find_peaks(frame,height=threshold,distance=dist)\n",
    "        #If there are any local maxima, add their position to the list of considered frames.\n",
    "        if len(localMax[0]):\n",
    "            frames[f]=localMax[0]\n",
    "        \n",
    "    trajectories={}\n",
    "    currentTrajectories=0\n",
    "    keys = np.array(list(frames.keys()))\n",
    "    \n",
    "    for frame in frames:\n",
    "        if flag:\n",
    "            break\n",
    "        curMax = np.array(frames[frame])\n",
    "        \n",
    "        # if len(curMax)>currentTrajectories:\n",
    "        #     for i in range(0,len(curMax)-currentTrajectories):\n",
    "        #             # print(len(curMax))\n",
    "        #             # print(i)\n",
    "        #             trajectories[currentTrajectories] = [(0,0)]\n",
    "        #             currentTrajectories+=1\n",
    "            \n",
    "        diff=np.zeros((currentTrajectories,len(curMax)))\n",
    "        if len(trajectories)>5 and False:\n",
    "            flag=1\n",
    "            break\n",
    "        for t in trajectories:\n",
    "            if flag:\n",
    "                break\n",
    "            # try:\n",
    "            #     shortTraj=np.median(trajectories[t][-keepTraj:-1])\n",
    "            # except:\n",
    "            shortTraj=trajectories[t][-1]\n",
    "            try:                  \n",
    "                diff[t] = (np.abs(np.array(curMax)-shortTraj[-1]))#**2\n",
    "                diffT=np.abs(frame-shortTraj[0])**2\n",
    "                diff[t]=np.sqrt(diff[t]+diffT)\n",
    "            except:\n",
    "                diff[t]=[150000]*len(curMax)\n",
    "              \n",
    "        notTakenTrajs=np.arange(0,len(curMax))\n",
    "        notUsedTrajs = np.arange(0,currentTrajectories)\n",
    "\n",
    "        for i in range(0,len(curMax)):\n",
    "            if flag:\n",
    "                break\n",
    "            try:\n",
    "                c=curMax[i]\n",
    "                #diffSum=np.sum(diff,1)\n",
    "                t=np.argmin(diff,axis=0)[i] #np.argmin(diffSum)#np.argmin(diff,axis=0)\n",
    "                if np.abs(frame-trajectories[t][-1][0])<trajTreshold and t in notUsedTrajs and np.abs(c-trajectories[t][-1][-1])<dist:# or trajectories[t]==[(0,0)]:\n",
    "    #                    if not t in usedT:\n",
    "                    notTakenTrajs=np.delete(notTakenTrajs,np.where(notTakenTrajs==i))\n",
    "                    notUsedTrajs=np.delete(notUsedTrajs,np.where(notUsedTrajs==t))\n",
    "                    trajectories[t]+=[(frame,c)]\n",
    "    #                        usedT=np.append(usedT,t)\n",
    "                       #diff =np.delete(diff,t,axis=0)   \n",
    "            except Exception as e: print(e)\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(0,len(notTakenTrajs)):\n",
    "            if flag:\n",
    "                break\n",
    "            trajectories[currentTrajectories] = [(frame,curMax[notTakenTrajs[0]])]\n",
    "            notTakenTrajs=np.delete(notTakenTrajs,0)\n",
    "            currentTrajectories+=1\n",
    "        \n",
    "    trajectories = [trajectories[t] for t in trajectories if len(trajectories[t])>keepTraj]\n",
    "    currentTrajectories=len(trajectories)\n",
    "    return trajectories,currentTrajectories,frames\n",
    "\n",
    "\n",
    "getTestTraj = Trajectory(I=[5,5],s=st,D=[0.95,0.95])\n",
    "#getTestTraj = Trajectory(I=[1,1.51],s=st)\n",
    "\n",
    "testImage=dt.FlipLR(dt.FlipUD(input_array() + init_particle_counter() \n",
    "                        + GenNoise(dX=lambda:.00001+.00003*np.random.rand(),\n",
    "                                                dA=0,#lambda:0+np.random.rand()*0.0001,\n",
    "                                                noise_lev=lambda:.0001,\n",
    "                                                biglam=lambda:0.6+.4*np.random.rand(),\n",
    "                                                bgnoiseCval=lambda:0.03+.02*np.random.rand(),\n",
    "                                                bgnoise=lambda:.08+.04*np.random.rand(),\n",
    "                                                bigx0=lambda: .1*np.random.randn(),\n",
    "                                                sinus_noise_amplitude=lambda: np.random.rand(),\n",
    "                                                freq =lambda: np.random.rand()*np.pi)\n",
    "                                    + getTestTraj**1\n",
    "                                    + PostProcess()))\n",
    "\n",
    "img,label = generate_training_batch(testImage,1) \n",
    "\n",
    "pred = GAN.predict(img).squeeze()\n",
    "label=label.squeeze()\n",
    "#%%current optimal \n",
    "plt.close('all')\n",
    "trajectories,currentTrajectories,frames=manTrack(pred,keepTraj=16,threshold=0.05,trajTreshold=128,dist=32)\n",
    "plt.figure()\n",
    "plt.imshow(pred,aspect='auto')    \n",
    "colourList=np.tile(plt.rcParams['axes.prop_cycle'].by_key()['color'],5)\n",
    "D=np.zeros(currentTrajectories)\n",
    "trajectoryLengths = [len(trajectories[i]) for i in range(0,currentTrajectories)]\n",
    "\n",
    "xtomu=0.029545454545454545*4\n",
    "deltaT=0.0049893\n",
    "for t in range(0,currentTrajectories):\n",
    "    time=np.array([trajectories[t][k][0] for k in range(1,len(trajectories[t]))])\n",
    "    x=[trajectories[t][k][1] for k in range(1,len(trajectories[t]))]  \n",
    "    plt.scatter(x,time,s=16,c=colourList[t])  \n",
    "    D[t] = np.mean([np.abs(trajectories[t][i][1]*xtomu-trajectories[t][i+1][1]*xtomu)**2 for i in range(1,len(trajectories[t])-1)])/2/deltaT#np.mean([np.abs(trajectories[t][i][1]-trajectories[t][i+1][1])**2 for i in range(0,len(trajectories[t])-1)])\n",
    "    D[t]+=np.mean([np.abs((trajectories[t][i+1][1]-trajectories[t][i+2][1])*(trajectories[t][i][1]-trajectories[t][i+1][1]))*xtomu**2 for i in range(1,len(trajectories[t])-2)])/deltaT\n",
    "    D[t]/=2\n",
    "\n",
    "print(D)\n",
    "print(trajectoryLengths)\n",
    "\n",
    "trajectories,currentTrajectories,frames=manTrack(label,keepTraj=16,threshold=0.05,trajTreshold=128,dist=32)\n",
    "plt.figure()\n",
    "plt.imshow(label,aspect='auto')    \n",
    "colourList=np.tile(plt.rcParams['axes.prop_cycle'].by_key()['color'],5)\n",
    "D=np.zeros(currentTrajectories)\n",
    "trajectoryLengths = [len(trajectories[i]) for i in range(0,currentTrajectories)]\n",
    "\n",
    "xtomu=0.029545454545454545*4\n",
    "deltaT=0.0049893\n",
    "for t in range(0,currentTrajectories):\n",
    "    time=np.array([trajectories[t][k][0] for k in range(1,len(trajectories[t]))])\n",
    "    x=[trajectories[t][k][1] for k in range(1,len(trajectories[t]))]  \n",
    "    plt.scatter(x,time,s=16,c=colourList[t])  \n",
    "    D[t] = np.mean([np.abs(trajectories[t][i][1]*xtomu-trajectories[t][i+1][1]*xtomu)**2 for i in range(1,len(trajectories[t])-1)])/2/deltaT#np.mean([np.abs(trajectories[t][i][1]-trajectories[t][i+1][1])**2 for i in range(0,len(trajectories[t])-1)])\n",
    "    D[t]+=np.mean([np.abs((trajectories[t][i+1][1]-trajectories[t][i+2][1])*(trajectories[t][i][1]-trajectories[t][i+1][1]))*xtomu**2 for i in range(1,len(trajectories[t])-2)])/deltaT\n",
    "    D[t]/=2\n",
    "\n",
    "print(D)\n",
    "print(trajectoryLengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM-GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "821125948ab5004286e99be1703d0915c654e8cc8587e2e263418884d5db29b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
